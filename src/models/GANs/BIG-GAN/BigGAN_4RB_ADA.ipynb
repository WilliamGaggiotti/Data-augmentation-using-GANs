{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import  pytorch_fid_wrapper as pfw\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import handshape_datasets as hd\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader \n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, transform=None):\n",
    "        super().__init__()\n",
    "        self.x = torch.from_numpy(dataset[0]).permute(0,3,1,2)\n",
    "        self.y = torch.from_numpy(dataset[1])\n",
    "        self.len = dataset[0].shape[0]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x[index]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \n",
    "    data = hd.load('PugeaultASL_A')\n",
    "\n",
    "    good_min = 40\n",
    "    good_classes = []\n",
    "    n_unique = len(np.unique(data[1]['y']))\n",
    "    for i in range(n_unique):\n",
    "        images = data[0][np.equal(i, data[1]['y'])]\n",
    "        if len(images) >= good_min:\n",
    "            good_classes = good_classes + [i]\n",
    "\n",
    "    x = data[0][np.in1d(data[1]['y'], good_classes)]\n",
    "    img_shape = x[0].shape\n",
    "    print(img_shape)\n",
    "    y = data[1]['y'][np.in1d(data[1]['y'], good_classes)]\n",
    "    y_dict = dict(zip(np.unique(y), range(len(np.unique(y)))))\n",
    "    y = np.vectorize(y_dict.get)(y)\n",
    "    classes = np.unique(y)\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    #escalo al rango  [1-,1]\n",
    "    x = (x.astype('float32') -127.5 ) / 127.5\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, train_size=0.8, test_size=0.2, stratify=y)\n",
    "\n",
    "    return n_classes, x_train, y_train, x_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_with_subject(subject_test=1):\n",
    "    \n",
    "    data = hd.load('PugeaultASL_A')\n",
    "\n",
    "    good_min = 40\n",
    "    good_classes = []\n",
    "    n_unique = len(np.unique(data[1]['y']))\n",
    "    for i in range(n_unique):\n",
    "        images = data[0][np.equal(i, data[1]['y'])]\n",
    "        if len(images) >= good_min:\n",
    "            good_classes = good_classes + [i]\n",
    "\n",
    "    x = data[0][np.in1d(data[1]['y'], good_classes)]\n",
    "\n",
    "    y = data[1]['y'][np.in1d(data[1]['y'], good_classes)]\n",
    "\n",
    "    s = data[1]['subjects'][np.in1d(data[1]['y'], good_classes)]\n",
    "\n",
    "    y_dict = dict(zip(np.unique(y), range(len(np.unique(y)))))\n",
    "    y = np.vectorize(y_dict.get)(y)\n",
    "\n",
    "    s_dict = dict(zip(np.unique(s), range(len(np.unique(s)))))\n",
    "    s = np.vectorize(s_dict.get)(s)\n",
    "\n",
    "    classes = np.unique(y)\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    x_train = x[np.not_equal(subject_test, s)]\n",
    "    y_train = y[np.not_equal(subject_test, s)]\n",
    "    x_test = x[np.equal(subject_test, s)]\n",
    "    y_test = y[np.equal(subject_test, s)]\n",
    "    \n",
    "    shuffler = np.random.permutation(x_train.shape[0])\n",
    "    x_train = x_train[shuffler]\n",
    "    y_train = y_train[shuffler]\n",
    "\n",
    "    shuffler_test = np.random.permutation(x_test.shape[0])\n",
    "    x_test = x_test[shuffler_test]\n",
    "    y_test = y_test[shuffler_test]\n",
    "    \n",
    "    #escalo al rango  [1-,1]\n",
    "    x_train = (x_train.astype('float32') -127.5 ) / 127.5\n",
    "    x_test = (x_test.astype('float32') -127.5 ) / 127.5\n",
    "\n",
    "\n",
    "    return n_classes, x_train, y_train, x_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lsa16_rotated():\n",
    "    images =[]\n",
    "    labels = []\n",
    "    i=0\n",
    "    path = \"/home/willys/tesis/Data-augmentation-using-GANs/datasets/rgb_black_background/\"\n",
    "    for filename in glob.glob(path+'*.png'):\n",
    "        #append iamge\n",
    "        image = Image.open(os.path.join(path, filename))\n",
    "        image_to_numpy=np.asarray(image)\n",
    "        images.append(image_to_numpy)\n",
    "        #create label and append\n",
    "        image_name = filename.split('/')[-1]\n",
    "        label = (int(image_name.split('_')[0]))\n",
    "        labels.append(label-1)\n",
    "\n",
    "    images = np.asarray(images)\n",
    "    labels = np.asarray(labels)\n",
    "    n_classes = len(np.unique(labels))\n",
    "    \n",
    "    #Desordeno el nuevo dataset\n",
    "    shuffler = np.random.permutation(images.shape[0])\n",
    "    images = images[shuffler]\n",
    "    labels = labels[shuffler]\n",
    "    \n",
    "    #escalo al rango  [1-,1]\n",
    "    images = (images.astype('float32') -127.5 ) / 127.5\n",
    "    #split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        images, labels, train_size=0.8, test_size=0.2, stratify=labels)\n",
    "    \n",
    "    return n_classes, x_train, y_train, x_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lsa16_rotated_with_subject(subject_test=1):\n",
    "    x_train =[]\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    i=0\n",
    "    path = \"/home/willys/tesis/Data-augmentation-using-GANs/datasets/rgb_black_background/\"\n",
    "    for filename in glob.glob(path+'*.png'):\n",
    "        #append iamge\n",
    "        image = Image.open(os.path.join(path, filename))\n",
    "        image_to_numpy=np.asarray(image)\n",
    "        \n",
    "        # get label and subject\n",
    "        image_name = filename.split('/')[-1]\n",
    "        label = (int(image_name.split('_')[0]))\n",
    "        subject = (int(image_name.split('_')[1]))\n",
    "        \n",
    "        if subject==subject_test:\n",
    "            x_test.append(image_to_numpy)\n",
    "            y_test.append(label-1)\n",
    "        else:\n",
    "            x_train.append(image_to_numpy)\n",
    "            y_train.append(label-1)        \n",
    "\n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    x_test = np.asarray(x_test)\n",
    "    y_test = np.asarray(y_test)\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    \n",
    "    #Desordeno los datos de train\n",
    "    shuffler_train = np.random.permutation(x_train.shape[0])\n",
    "    x_train = x_train[shuffler_train]\n",
    "    y_train = y_train[shuffler_train]\n",
    "    \n",
    "    #Desordeno los datos de train\n",
    "    shuffler_test = np.random.permutation(x_test.shape[0])\n",
    "    x_test = x_test[shuffler_test]\n",
    "    y_test = y_test[shuffler_test]\n",
    "    \n",
    "    #escalo al rango  [1-,1]\n",
    "    x_train = (x_train.astype('float32') -127.5 ) / 127.5\n",
    "    x_test = (x_test.astype('float32') -127.5 ) / 127.5\n",
    "    \n",
    "    return n_classes, x_train, y_train, x_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonal_regularization(weight):\n",
    "    '''\n",
    "    Function for computing the orthogonal regularization term for a given weight matrix.\n",
    "    '''\n",
    "    weight = weight.flatten(1)\n",
    "    return torch.norm(\n",
    "        torch.dot(weight, weight) * (torch.ones_like(weight) - torch.eye(weight.shape[0]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=16, size=(3, 32, 32), nrow=4, show=True, save=False, \n",
    "                       path=''):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassConditionalBatchNorm2d(nn.Module):\n",
    "    '''\n",
    "    ClassConditionalBatchNorm2d Class\n",
    "    Values:\n",
    "    in_channels: the dimension of the class embedding (c) + noise vector (z), a scalar\n",
    "    out_channels: the dimension of the activation tensor to be normalized, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
    "        self.class_scale_transform = nn.utils.spectral_norm(nn.Linear(in_channels, out_channels, bias=False))\n",
    "        self.class_shift_transform = nn.utils.spectral_norm(nn.Linear(in_channels, out_channels, bias=False))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        normalized_image = self.bn(x)\n",
    "        class_scale = (1 + self.class_scale_transform(y))[:, :, None, None]\n",
    "        class_shift = self.class_shift_transform(y)[:, :, None, None]\n",
    "        transformed_image = class_scale * normalized_image + class_shift\n",
    "        return transformed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    '''\n",
    "    AttentionBlock Class\n",
    "    Values:\n",
    "    channels: number of channels in input\n",
    "    '''\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "\n",
    "        self.theta = nn.utils.spectral_norm(nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=False))\n",
    "        self.phi = nn.utils.spectral_norm(nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=False))\n",
    "        self.g = nn.utils.spectral_norm(nn.Conv2d(channels, channels // 2, kernel_size=1, padding=0, bias=False))\n",
    "        self.o = nn.utils.spectral_norm(nn.Conv2d(channels // 2, channels, kernel_size=1, padding=0, bias=False))\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        spatial_size = x.shape[2] * x.shape[3]\n",
    "\n",
    "        # Apply convolutions to get query (theta), key (phi), and value (g) transforms\n",
    "        theta = self.theta(x)\n",
    "        phi = F.max_pool2d(self.phi(x), kernel_size=2)\n",
    "        g = F.max_pool2d(self.g(x), kernel_size=2)\n",
    "\n",
    "        # Reshape spatial size for self-attention\n",
    "        theta = theta.view(-1, self.channels // 8, spatial_size)\n",
    "        phi = phi.view(-1, self.channels // 8, spatial_size // 4)\n",
    "        g = g.view(-1, self.channels // 2, spatial_size // 4)\n",
    "\n",
    "        # Compute dot product attention with query (theta) and key (phi) matrices\n",
    "        beta = F.softmax(torch.bmm(theta.transpose(1, 2), phi), dim=-1)\n",
    "\n",
    "        # Compute scaled dot product attention with value (g) and attention (beta) matrices\n",
    "        o = self.o(torch.bmm(g, beta.transpose(1, 2)).view(-1, self.channels // 2, x.shape[2], x.shape[3]))\n",
    "\n",
    "        # Apply gain and residual\n",
    "        return self.gamma * o + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GResidualBlock(nn.Module):\n",
    "    '''\n",
    "    GResidualBlock Class\n",
    "    Values:\n",
    "    c_dim: the dimension of conditional vector [c, z], a scalar\n",
    "    in_channels: the number of channels in the input, a scalar\n",
    "    out_channels: the number of channels in the output, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, c_dim, in_channels, out_channels, scale_factor=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "        self.bn1 = ClassConditionalBatchNorm2d(c_dim, in_channels)\n",
    "        self.bn2 = ClassConditionalBatchNorm2d(c_dim, out_channels)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.upsample_fn = nn.Upsample(scale_factor=scale_factor) # upsample occurs in every gblock\n",
    "\n",
    "        self.mixin = (in_channels != out_channels)\n",
    "        if self.mixin:\n",
    "            self.conv_mixin = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # h := upsample(x, y)\n",
    "        h = self.bn1(x, y)\n",
    "        h = self.activation(h)\n",
    "        h = self.upsample_fn(h)\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        # h := conv(h, y)\n",
    "        h = self.bn2(h, y)\n",
    "        h = self.activation(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        # x := upsample(x)\n",
    "        x = self.upsample_fn(x)\n",
    "        if self.mixin:\n",
    "            x = self.conv_mixin(x)\n",
    "\n",
    "        return h + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "    z_dim: the dimension of random noise sampled, a scalar\n",
    "    shared_dim: the dimension of shared class embeddings, a scalar\n",
    "    base_channels: the number of base channels, a scalar\n",
    "    bottom_width: the height/width of image before it gets upsampled, a scalar\n",
    "    n_classes: the number of image classes, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base_channels=96, bottom_width=4, z_dim=120, shared_dim=128, n_classes=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        n_chunks = 5    # 4 (generator blocks) + 1 (generator input)\n",
    "        self.z_chunk_size = z_dim // n_chunks\n",
    "        self.z_dim = z_dim\n",
    "        self.shared_dim = shared_dim\n",
    "        self.bottom_width = bottom_width\n",
    "\n",
    "        # No spectral normalization on embeddings, which authors observe to cripple the generator\n",
    "        self.shared_emb = nn.Embedding(n_classes, shared_dim)\n",
    "\n",
    "        self.proj_z = nn.Linear(self.z_chunk_size, 16 * base_channels * bottom_width ** 2)\n",
    "\n",
    "        # Can't use one big nn.Sequential since we are adding class+noise at each block\n",
    "        self.g_blocks = nn.ModuleList([\n",
    "\n",
    "            nn.ModuleList([\n",
    "                GResidualBlock(shared_dim + self.z_chunk_size, 16 * base_channels, 8 * base_channels),\n",
    "                AttentionBlock(8 * base_channels),\n",
    "            ]),\n",
    "             nn.ModuleList([\n",
    "                GResidualBlock(shared_dim + self.z_chunk_size, 8 * base_channels, 4 * base_channels),\n",
    "                AttentionBlock(4 * base_channels),\n",
    "            ]),\n",
    "            nn.ModuleList([\n",
    "                GResidualBlock(shared_dim + self.z_chunk_size, 4 * base_channels, 2 * base_channels),\n",
    "                AttentionBlock(2 * base_channels),\n",
    "            ]),\n",
    "            nn.ModuleList([\n",
    "                GResidualBlock(shared_dim + self.z_chunk_size, 2 * base_channels, base_channels),\n",
    "                AttentionBlock(base_channels),\n",
    "            ]),\n",
    "        ])\n",
    "        self.proj_o = nn.Sequential(\n",
    "            nn.BatchNorm2d(base_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.utils.spectral_norm(nn.Conv2d(base_channels, 3, kernel_size=1, padding=0)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def loss (self, disc_fake_pred):\n",
    "        \n",
    "        return - disc_fake_pred.mean()\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        '''\n",
    "        z: random noise with size self.z_dim\n",
    "        y: class embeddings with size self.shared_dim\n",
    "            = NOTE =\n",
    "            y should be class embeddings from self.shared_emb, not the raw class labels\n",
    "        '''\n",
    "        # Chunk z and concatenate to shared class embeddings\n",
    "        zs = torch.split(z, self.z_chunk_size, dim=1)\n",
    "        z = zs[0]\n",
    "        ys = [torch.cat([y, z], dim=1) for z in zs[1:]]\n",
    "\n",
    "        # Project noise and reshape to feed through generator blocks\n",
    "        h = self.proj_z(z)\n",
    "        h = h.view(h.size(0), -1, self.bottom_width, self.bottom_width)\n",
    "\n",
    "        # Feed through generator blocks\n",
    "        for idx, g_block in enumerate(self.g_blocks):\n",
    "            h = g_block[0](h, ys[idx])\n",
    "            h = g_block[1](h)\n",
    "\n",
    "        # Project to 3 RGB channels with tanh to map values to [-1, 1]\n",
    "        h = self.proj_o(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DResidualBlock(nn.Module):\n",
    "    '''\n",
    "    DResidualBlock Class\n",
    "    Values:\n",
    "    in_channels: the number of channels in the input, a scalar\n",
    "    out_channels: the number of channels in the output, a scalar\n",
    "    downsample: whether to apply downsampling\n",
    "    use_preactivation: whether to apply an activation function before the first convolution\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, downsample=True, use_preactivation=False, \n",
    "                 downsample_scale=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.use_preactivation = use_preactivation  # apply preactivation in all except first dblock\n",
    "\n",
    "        self.downsample = downsample    # downsample occurs in all except last dblock\n",
    "        if downsample:\n",
    "            self.downsample_fn = nn.AvgPool2d(downsample_scale)\n",
    "        self.mixin = (in_channels != out_channels) or downsample\n",
    "        if self.mixin:\n",
    "            self.conv_mixin = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0))\n",
    "\n",
    "    def _residual(self, x):\n",
    "        if self.use_preactivation:\n",
    "            if self.mixin:\n",
    "                x = self.conv_mixin(x)\n",
    "            if self.downsample:\n",
    "                x = self.downsample_fn(x)\n",
    "        else:\n",
    "            if self.downsample:\n",
    "                x = self.downsample_fn(x)\n",
    "            if self.mixin:\n",
    "                x = self.conv_mixin(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply preactivation if applicable\n",
    "        if self.use_preactivation:\n",
    "            h = F.relu(x)\n",
    "        else:\n",
    "            h = x\n",
    "\n",
    "        h = self.conv1(h)\n",
    "        h = self.activation(h)\n",
    "        if self.downsample:\n",
    "            h = self.downsample_fn(h)\n",
    "\n",
    "        return h + self._residual(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "    base_channels: the number of base channels, a scalar\n",
    "    n_classes: the number of image classes, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base_channels=96, n_classes=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        # For adding class-conditional evidence\n",
    "        self.shared_emb = nn.utils.spectral_norm(nn.Embedding(n_classes, 8 * base_channels))\n",
    "\n",
    "        self.d_blocks = nn.Sequential(\n",
    "            DResidualBlock(3, base_channels, downsample=True, use_preactivation=False),\n",
    "            AttentionBlock(base_channels),\n",
    "\n",
    "            DResidualBlock(base_channels, 2 * base_channels, downsample=True, use_preactivation=True),\n",
    "            AttentionBlock(2 * base_channels),\n",
    "\n",
    "            DResidualBlock(2 * base_channels, 4 * base_channels, downsample=True, use_preactivation=True,\n",
    "                          downsample_scale=2),\n",
    "            AttentionBlock(4 * base_channels),\n",
    "            \n",
    "            DResidualBlock(4 * base_channels, 8 * base_channels, downsample=True, use_preactivation=True,\n",
    "                          downsample_scale=2),\n",
    "            AttentionBlock(8 * base_channels),\n",
    "\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.proj_o = nn.utils.spectral_norm(nn.Linear(8 * base_channels, 1))\n",
    "        \n",
    "    def loss(self, disc_real_pred, disc_fake_pred):\n",
    "        \n",
    "        d_loss_fake = torch.nn.ReLU()(1.0 + disc_fake_pred).mean()\n",
    "        d_loss_real = torch.nn.ReLU()(1.0 - disc_real_pred).mean()\n",
    "        \n",
    "        return d_loss_real + d_loss_fake\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        h = self.d_blocks(x)\n",
    "        h = torch.sum(h, dim=[2, 3])\n",
    "\n",
    "        # Class-unconditional output\n",
    "        uncond_out = self.proj_o(h)\n",
    "        if y is None:\n",
    "            return uncond_out\n",
    "\n",
    "        # Class-conditional output\n",
    "        cond_out = torch.sum(self.shared_emb(y) * h, dim=1, keepdim=True)\n",
    "        \n",
    "        return uncond_out + cond_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomApplyEach(nn.Module):\n",
    "    def __init__(self, transforms, p):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, img):\n",
    "        for t in self.transforms:\n",
    "            if self.p > torch.rand(1, device='cuda'):\n",
    "                img = t(img)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        format_string += '\\n    p={}'.format(self.p)\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, base_channels, z_dim, shared_dim, n_classes,\n",
    "          generator, discriminator, gen_opt, disc_opt, epochs, weights_dir, summary_fid):\n",
    "    \n",
    "    cur_step = 0\n",
    "    min_fids = np.array([2000,2000,2000])\n",
    "    augmentation_transforms = [\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomAffine(degrees=15, fill=0),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), fill=0),\n",
    "        transforms.RandomAffine(degrees=0, scale=(0.7, 1.3), fill=0),\n",
    "        transforms.ColorJitter(brightness=0.5),\n",
    "        transforms.ColorJitter(contrast=0.5),\n",
    "        transforms.ColorJitter(saturation=0.5),\n",
    "        transforms.ColorJitter(hue=0.2),\n",
    "    ]\n",
    "    p = torch.tensor(0.0, device=device)\n",
    "    ada_target = 0.6\n",
    "    update_iteration = 8\n",
    "    adjustment_size = 250000 # number of images to reach p=1\n",
    "    augmentation = RandomApplyEach(augmentation_transforms, p).to(device)\n",
    "    ada_buf = torch.tensor([0.0, 0.0], device=device)\n",
    "    fakes = torch.tensor([], device='cpu')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('##############################')\n",
    "        print('#epoch: {}'.format(epoch))\n",
    "        print('##############################')\n",
    "        \n",
    "        for batch_ndx, sample in enumerate(loader):\n",
    "            real, labels = sample[0], sample[1]\n",
    "            batch_size = len(real)\n",
    "            real = real.to(device)\n",
    "            real_augmented = augmentation(real)\n",
    "\n",
    "            for i in range(2):\n",
    "                ### Update discriminator ###\n",
    "                # Zero out the discriminator gradients\n",
    "                disc_opt.zero_grad()\n",
    "                # Get noise corresponding to the current batch_size \n",
    "                z = torch.randn(batch_size, z_dim, device=device)       # Generate random noise (z)\n",
    "                y = labels.to(device).long()    # Generate a batch of labels (y), one for each class\n",
    "                y_emb = generator.shared_emb(y)                         # Retrieve class embeddings (y_emb) from generator\n",
    "                fake = generator(z, y_emb)\n",
    "                fake = augmentation(fake.detach())\n",
    "\n",
    "                disc_fake_pred = discriminator(fake, y)  \n",
    "                disc_real_pred = discriminator(real_augmented, y)\n",
    "                \n",
    "                #loss\n",
    "                disc_loss = discriminator.loss(disc_real_pred, disc_fake_pred)\n",
    "                # Update gradients\n",
    "                disc_loss.backward(retain_graph=True)\n",
    "                # Update optimizer\n",
    "                disc_opt.step()\n",
    "\n",
    "\n",
    "\n",
    "            ### Update generator ###\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            fake = generator(z, y_emb)\n",
    "            fake = augmentation(fake)\n",
    "            disc_fake_pred = discriminator(fake, y)  \n",
    "            #loss\n",
    "            gen_loss =  generator.loss(disc_fake_pred)\n",
    "            # Update gradients\n",
    "            gen_loss.backward()\n",
    "            # Update optimizer\n",
    "            gen_opt.step()            \n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            cur_step +=1\n",
    "            fakes = torch.cat((fakes, fake.to('cpu')))\n",
    "            \n",
    "            if cur_step % update_iteration == 0:\n",
    "                # Adaptive Data Augmentation\n",
    "                pred_signs, n_pred = ada_buf\n",
    "                r_t = pred_signs / n_pred\n",
    "\n",
    "                sign = 1 if r_t > ada_target else -1\n",
    "\n",
    "                augmentation.p = torch.clamp(augmentation.p + (sign * n_pred / adjustment_size), min=0, max=1)\n",
    "\n",
    "                ada_buf = ada_buf * 0\n",
    "            \n",
    "            if cur_step % 500 == 0:\n",
    "                print('===========================================================================')\n",
    "                show_tensor_images(real)\n",
    "                show_tensor_images(fake)\n",
    "                val_fid = pfw.fid(fakes, real_m=real_m, real_s=real_s)\n",
    "                fakes = torch.tensor([], device='cpu')\n",
    "                print('FID: {}'.format(val_fid))\n",
    "                print('augmentation p: {}'.format(augmentation.p))\n",
    "                if (val_fid < min_fids).any():\n",
    "                    idx = min_fids.argmax()\n",
    "                    min_fids[idx] = val_fid\n",
    "                    weights_dir_specific = weights_dir+'weights_{}/'.format(idx)\n",
    "                    if not os.path.exists(weights_dir_specific):\n",
    "                        os.makedirs(weights_dir_specific)\n",
    "                    torch.save(generator.state_dict(), (weights_dir_specific+'gen.state_dict'))\n",
    "                    path_image = weights_dir_specific+'images_gen.png'\n",
    "                    show_tensor_images(fake, show=False, save=True, path=path_image)\n",
    "                print('===========================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in range(0,1):\n",
    "    \n",
    "    device = 'cuda'\n",
    "    #Creo los paths \n",
    "    var_dir = './saved_variables/PugeaultASL_A_ADA/subject_{}'.format(subject)\n",
    "    if not os.path.exists(var_dir):\n",
    "        os.makedirs(var_dir)\n",
    "    weights_dir = 'generators_weights/PugeaultASL_A_ADA/subject_{}'.format(subject)\n",
    "    if not os.path.exists(weights_dir):\n",
    "        os.makedirs(weights_dir)\n",
    "    data_dir = 'numpy_data/PugeaultASL_A_ADA/'\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    summary_fid = open(weights_dir+\"summary_fid.txt\", \"a\")\n",
    "    \n",
    "    #charge data\n",
    "    n_classes, x_train, y_train, x_test, y_test = load_dataset_with_subject(subject_test=subject)\n",
    "    \n",
    "    batch_size = 32\n",
    "    dataset = CustomDataset((x_train,y_train))#, transform=transforms.ToTensor())\n",
    "    print(\"Creating dataset object\")\n",
    "    \n",
    "    loader = data.DataLoader(dataset, shuffle=True, batch_size=batch_size, pin_memory=True, num_workers=4)\n",
    "\n",
    "    # Initialize models\n",
    "    print(\"Creating models\")\n",
    "    base_channels = 96\n",
    "    z_dim = 120\n",
    "    shared_dim = 128\n",
    "    generator = Generator(base_channels=base_channels, bottom_width=2, z_dim=z_dim, shared_dim=shared_dim, n_classes=n_classes).to(device)\n",
    "    discriminator = Discriminator(base_channels=base_channels, n_classes=n_classes).to(device)\n",
    "\n",
    "    # Initialize weights orthogonally\n",
    "    for module in generator.modules():\n",
    "        if (isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, nn.Embedding)):\n",
    "            nn.init.orthogonal_(module.weight)\n",
    "    for module in discriminator.modules():\n",
    "        if (isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, nn.Embedding)):\n",
    "            nn.init.orthogonal_(module.weight)\n",
    "\n",
    "    # Initialize optimizers\n",
    "    gen_opt = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.0, 0.999), eps=1e-6)\n",
    "    disc_opt = torch.optim.Adam(discriminator.parameters(), lr=4e-4, betas=(0.0, 0.999), eps=1e-6)\n",
    "        \n",
    "    # Setup FID\n",
    "    print(\"Calculating FID parameters\")\n",
    "    pfw.set_config(batch_size=batch_size, device=device)\n",
    "    if os.path.isfile(var_dir+'fid_stats_ASL.pkl'):\n",
    "        with open(var_dir+'fid_stats_ASL.pkl', 'rb') as f:\n",
    "            real_m, real_s = pickle.load(f)        \n",
    "    else:\n",
    "        real_m, real_s = pfw.get_stats((dataset.x))\n",
    "        with open(var_dir+'fid_stats_ASL.pkl', 'wb') as f:\n",
    "            pickle.dump([real_m, real_s], f)\n",
    "    print(\"FID parameters calculated!\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    train(loader, base_channels, z_dim, shared_dim, n_classes,\n",
    "          generator, discriminator, gen_opt, disc_opt, 10, weights_dir, summary_fid)\n",
    "    \n",
    "    summary_fid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataset.x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-daughter",
   "metadata": {},
   "source": [
    "## Training GANS with reduce dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_reduces = [0.05, 0.01, 0.005, 0.004]\n",
    "n_classes, _, _, _, _ = load_lsa16_rotated()\n",
    "for factor_reduce in factor_reduces:\n",
    "    device = 'cuda'\n",
    "    #Creo los paths \n",
    "    weights_dir = 'generators_weights/lsa_16_rotated_reduce/{}/'.format(str(factor_reduce))\n",
    "    if not os.path.exists(weights_dir):\n",
    "        os.makedirs(weights_dir)\n",
    "    data_dir = 'numpy_data/lsa_16_rotated_reduce/'\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "        \n",
    "    #Creo un txt para registrar los FID\n",
    "    summary_fid = open(weights_dir+\"summary_fid.txt\", \"a\") \n",
    "    \n",
    "    #load data\n",
    "    x_train = np.load(data_dir+'x_train.npy')\n",
    "    y_train = np.load(data_dir+'y_train.npy')\n",
    "    \n",
    "    #spliteo segun el factor_reduce\n",
    "    x_train, _, y_train, _ = train_test_split(\n",
    "                                            x_train, y_train, train_size=factor_reduce, \n",
    "                                            test_size=(1-factor_reduce),stratify=y_train)\n",
    "    \n",
    "    dataset = CustomDataset((x_train,y_train), transform=transforms.ToTensor())\n",
    "    batch_size = 16\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, num_workers=4)\n",
    "    \n",
    "   # Initialize models\n",
    "    base_channels = 96\n",
    "    z_dim = 120  \n",
    "    shared_dim = 128\n",
    "    generator = Generator(base_channels=base_channels, bottom_width=4, z_dim=z_dim, shared_dim=shared_dim, n_classes=n_classes).to(device)\n",
    "    discriminator = Discriminator(base_channels=base_channels, n_classes=n_classes).to(device)\n",
    "\n",
    "    # Initialize weights orthogonally\n",
    "    for module in generator.modules():\n",
    "        if (isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, nn.Embedding)):\n",
    "            nn.init.orthogonal_(module.weight)\n",
    "    for module in discriminator.modules():\n",
    "        if (isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, nn.Embedding)):\n",
    "            nn.init.orthogonal_(module.weight)\n",
    "\n",
    "    # Initialize optimizers\n",
    "    gen_opt = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.0, 0.999), eps=1e-6)\n",
    "    disc_opt = torch.optim.Adam(discriminator.parameters(), lr=4e-4, betas=(0.0, 0.999), eps=1e-6)\n",
    "    \n",
    "    train(loader, base_channels, z_dim, shared_dim, n_classes,\n",
    "          generator, discriminator, gen_opt, disc_opt, 20000, weights_dir, summary_fid)\n",
    "    \n",
    "    summary_fid.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-income",
   "metadata": {},
   "source": [
    "## Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in range(2,5):\n",
    "    print('##############')\n",
    "    print('#  subject {} #'.format(q))\n",
    "    print('##############')\n",
    "    weights_path = 'generators_weights/PugeaultASL_A_transforms/subject_{}/'.format(q)\n",
    "    numpy_data_path = 'numpy_data/PugeaultASL_A_transforms/subject_{}/'.format(q)\n",
    "    if not os.path.exists(numpy_data_path):\n",
    "        os.makedirs(numpy_data_path)\n",
    "    total_images = 1792\n",
    "    batch_size = 64\n",
    "    n_classes, x_train, y_train, _, _ = load_dataset_with_subject(subject_test=q)\n",
    "    # Initialize models\n",
    "    base_channels = 96\n",
    "    z_dim = 120  \n",
    "    shared_dim = 128\n",
    "    device = 'cuda'\n",
    "    generator = Generator(base_channels=base_channels, bottom_width=4, \n",
    "                          z_dim=z_dim, shared_dim=shared_dim, n_classes=n_classes).to(device)\n",
    "    \n",
    "    #100% imagenes generadas\n",
    "    x_aug_100 = []\n",
    "    y_aug_100 = []\n",
    "    \n",
    "    for k in range(3):\n",
    "        specific_weights_path = weights_path+('weights_{}/gen.state_dict'.format(k))\n",
    "        generator.load_state_dict(torch.load(specific_weights_path))\n",
    "        generator.eval()\n",
    "        \n",
    "        for j in range(total_images//batch_size):\n",
    "            for i in range(n_classes):\n",
    "\n",
    "                #creo imagenes falsas y las mergeo\n",
    "                z = torch.randn(batch_size, z_dim, device=device)                 # Generate random noise (z)\n",
    "                y = (torch.ones(batch_size) * i).to(device).long()    # Generate a batch of labels (y), one for each class\n",
    "                y_emb = generator.shared_emb(y)                                  # Retrieve class embeddings (y_emb) from generator\n",
    "                fake = generator(z, y_emb)\n",
    "\n",
    "                #acomodo las dimensiones, escala y tipo\n",
    "                image_unflat = fake.detach().cpu()\n",
    "                final_images = image_unflat.permute(0,2, 3, 1).numpy()\n",
    "\n",
    "                x_aug_100.append(final_images)\n",
    "                y_tmp = np.ones(batch_size,)* i\n",
    "                y_aug_100.append(y_tmp)\n",
    "                    \n",
    "    \n",
    "    #dataset 100% GAN\n",
    "    x_aug_100 = (np.asarray(x_aug_100)).reshape((-1,32,32,3))\n",
    "    y_aug_100 = (np.asarray(y_aug_100)).reshape((-1))\n",
    "    \n",
    "    #dataset 50/50\n",
    "    x_aug_50, _, y_aug_50, _ = train_test_split(\n",
    "        x_aug_100, y_aug_100, train_size=0.5, test_size=0.5, stratify=y_aug_100)\n",
    "    \n",
    "    x_aug_50 =np.concatenate((x_aug_50, x_train), axis=0)\n",
    "    y_aug_50 = np.concatenate((y_aug_50, y_train), axis=0)\n",
    "    \n",
    "    #dataset 75/25 (GAN/normal)\n",
    "    '''Como el dataset normal tiene la mitad de datos que x_aug_100, el 25% de datos normales \n",
    "    con respecto al dataset GAN equivale a la mitad del dataset original'''\n",
    "    x_aug_75, _, y_aug_75, _ = train_test_split(\n",
    "        x_aug_100, y_aug_100, train_size=0.75, test_size=0.25, stratify=y_aug_100)\n",
    "    \n",
    "    x_train_25, _, y_train_25, _ = train_test_split(\n",
    "        x_train, y_train, train_size=0.5, test_size=0.5, stratify=y_train)\n",
    "    \n",
    "    x_aug_75 =np.concatenate((x_aug_75, x_train_25), axis=0)\n",
    "    y_aug_75 = np.concatenate((y_aug_75, y_train_25), axis=0)\n",
    "    \n",
    "    #dataset 25/75 (GAN/normal)\n",
    "    '''Como el dataset normal tiene la mitad de datos que x_aug_100, el 25% de datos GAN \n",
    "    con respecto al dataset original es del 12.5%'''\n",
    "    x_aug_25, _, y_aug_25, _ = train_test_split(\n",
    "        x_aug_100, y_aug_100, train_size=0.125, test_size=0.875, stratify=y_aug_100)\n",
    "    \n",
    "    x_train_75, _, y_train_75, _ = train_test_split(\n",
    "        x_train, y_train, train_size=0.75, test_size=0.25, stratify=y_train)\n",
    "    \n",
    "    x_aug_25 =np.concatenate((x_aug_25, x_train_75), axis=0)\n",
    "    y_aug_25 = np.concatenate((y_aug_25, y_train_75), axis=0)\n",
    "    \n",
    "    #Desordeno el nuevo dataset 100% GAN\n",
    "    shuffler = np.random.permutation(x_aug_100.shape[0])\n",
    "    x_aug_100 = x_aug_100[shuffler]\n",
    "    y_aug_100 = y_aug_100[shuffler]\n",
    "    print(\" dataset 100% GAN shape: {}, y final:{}\".format(x_aug_100.shape,y_aug_100.shape))\n",
    "\n",
    "    #Desordeno el nuevo dataset 50/50\n",
    "    shuffler = np.random.permutation(x_aug_50.shape[0])\n",
    "    x_aug_50 = x_aug_50[shuffler]\n",
    "    y_aug_50 = y_aug_50[shuffler]\n",
    "    print(\" dataset 50/50 shape: {}, y final:{}\".format(x_aug_50.shape,y_aug_50.shape))\n",
    "\n",
    "    #Desordeno el nuevo dataset 75/25 (GAN/normal)\n",
    "    shuffler = np.random.permutation(x_aug_75.shape[0])\n",
    "    x_aug_75 = x_aug_75[shuffler]\n",
    "    y_aug_75 = y_aug_75[shuffler]\n",
    "    print(\" dataset 75/25 shape: {}, y final:{}\".format(x_aug_75.shape,y_aug_75.shape))\n",
    "\n",
    "    #Desordeno el nuevo dataset 25/75 (GAN/normal)\n",
    "    shuffler = np.random.permutation(x_aug_25.shape[0])\n",
    "    x_aug_25 = x_aug_25[shuffler]\n",
    "    y_aug_25 = y_aug_25[shuffler]\n",
    "    print(\" dataset 25/75 shape: {}, y final:{}\".format(x_aug_25.shape,y_aug_25.shape))\n",
    "\n",
    "    #Guardamos los datos mergeados\n",
    "    np.save(numpy_data_path+'x_aug_100',x_aug_100)\n",
    "    np.save(numpy_data_path+'y_aug_100',y_aug_100)\n",
    "    np.save(numpy_data_path+'x_aug_50',x_aug_50)\n",
    "    np.save(numpy_data_path+'y_aug_50',y_aug_50)\n",
    "    np.save(numpy_data_path+'x_aug_75',x_aug_75)\n",
    "    np.save(numpy_data_path+'y_aug_75',y_aug_75)\n",
    "    np.save(numpy_data_path+'x_aug_25',x_aug_25)\n",
    "    np.save(numpy_data_path+'y_aug_25',y_aug_25)\n",
    "\n",
    "    #limpiamos memoria\n",
    "    torch.cuda.empty_cache()\n",
    "    del generator, x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-privilege",
   "metadata": {},
   "source": [
    "## Generating Data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduces = [0.005]\n",
    "for factor_reduce in reduces:\n",
    "    print('##############')\n",
    "    print('#  Factor_reduce {} #'.format(factor_reduce))\n",
    "    print('##############')\n",
    "    \n",
    "    weights_path = 'generators_weights/lsa_16_rotated_reduce/{}/'.format(factor_reduce)\n",
    "    numpy_data_path = '/media/willys/MULTIBOOT/tesis/numpy_data/lsa_16_rotated_reduce/{}/'.format(factor_reduce)\n",
    "    if not os.path.exists(numpy_data_path):\n",
    "        os.makedirs(numpy_data_path)\n",
    "    \n",
    "    #original dataset\n",
    "    x_train = np.load('numpy_data/lsa_16_rotated_reduce/x_train.npy')\n",
    "    y_train = np.load('numpy_data/lsa_16_rotated_reduce/y_train.npy')\n",
    "    #splt\n",
    "    x_train, _, y_train, _ = train_test_split(\n",
    "        x_train, y_train, train_size=factor_reduce, stratify=y_train)\n",
    "    print('x_train shape: {}, y_train shape{}'.format(x_train.shape,y_train.shape))\n",
    "    \n",
    "    n_classes, _, _, _, _ = load_lsa16_rotated()\n",
    "    total_images_of_generator = (x_train.shape[0]//n_classes)+1\n",
    "    \n",
    "    batch_size = 16 if total_images_of_generator > 16 else int(total_images_of_generator)\n",
    "    print('batch_size: {}'.format(batch_size))\n",
    "    # Initialize models\n",
    "    base_channels = 96\n",
    "    z_dim = 120  \n",
    "    shared_dim = 128\n",
    "    device = 'cuda'\n",
    "    generator = Generator(base_channels=base_channels, bottom_width=4, \n",
    "                          z_dim=z_dim, shared_dim=shared_dim, n_classes=n_classes).to(device)\n",
    "    \n",
    "    #100% imagenes generadas\n",
    "    x_gan_300 = []\n",
    "    y_gan_300 = []\n",
    "    \n",
    "    for k in range(3):\n",
    "        specific_weights_path = weights_path+('weights_{}/gen.state_dict'.format(k))\n",
    "        generator.load_state_dict(torch.load(specific_weights_path))\n",
    "        generator.eval()\n",
    "        \n",
    "        for j in range(round(total_images_of_generator/batch_size)):\n",
    "            for i in range(n_classes):\n",
    "\n",
    "                #creo imagenes falsas y las mergeo\n",
    "                z = torch.randn(batch_size, z_dim, device=device)                 # Generate random noise (z)\n",
    "                y = (torch.ones(batch_size) * i).to(device).long()    # Generate a batch of labels (y), one for each class\n",
    "                y_emb = generator.shared_emb(y)                                  # Retrieve class embeddings (y_emb) from generator\n",
    "                fake = generator(z, y_emb)\n",
    "\n",
    "                #acomodo las dimensiones, escala y tipo\n",
    "                image_unflat = fake.detach().cpu()\n",
    "                final_images = image_unflat.permute(0,2, 3, 1).numpy()\n",
    "\n",
    "                x_gan_300.append(final_images)\n",
    "                y_tmp = np.ones(batch_size,)* i\n",
    "                y_gan_300.append(y_tmp)\n",
    "                    \n",
    "    \n",
    "    #300% data gen\n",
    "    x_gan_300 = (np.asarray(x_gan_300)).reshape((-1,64,64,3))\n",
    "    y_gan_300 = (np.asarray(y_gan_300)).reshape((-1))\n",
    "    print('shape x_gan_300: {}, y_gan_300: {}'.format(x_gan_300.shape, y_gan_300.shape))\n",
    "    \n",
    "    #200% y 100% data gen \n",
    "    x_gan_200, x_gan_100, y_gan_200, y_gan_100 = train_test_split(\n",
    "        x_gan_300, y_gan_300, train_size=0.66, test_size=0.33, stratify=y_gan_300)\n",
    "    \n",
    "    #dataset original + 25% datos generados\n",
    "    '''x_gan_25, _, y_gan_25, _ = train_test_split(\n",
    "        x_gan_100, y_gan_100, train_size=int(round(x_train.shape[0]/4)),\n",
    "        stratify=y_gan_100)\n",
    "    \n",
    "    x_aug_25 = np.concatenate((x_gan_25, x_train), axis=0)\n",
    "    y_aug_25 = np.concatenate((y_gan_25, y_train), axis=0)'''\n",
    "    \n",
    "    #dataset original + 50% datos generados\n",
    "    x_gan_50, _, y_gan_50, _ = train_test_split(\n",
    "        x_gan_100, y_gan_100, train_size=int(round(x_train.shape[0]/2)), stratify=y_gan_100)\n",
    "    \n",
    "    x_aug_50 =np.concatenate((x_gan_50, x_train), axis=0)\n",
    "    y_aug_50 = np.concatenate((y_gan_50, y_train), axis=0)\n",
    "    \n",
    "    #dataset original + 75% datos generados\n",
    "    x_gan_75, _, y_gan_75, _ = train_test_split(\n",
    "        x_gan_100, y_gan_100, train_size=(int(round(x_train.shape[0]*3/4))), \n",
    "        stratify=y_gan_100)\n",
    "    \n",
    "    x_aug_75 =np.concatenate((x_gan_75, x_train), axis=0)\n",
    "    y_aug_75 = np.concatenate((y_gan_75, y_train), axis=0)\n",
    "    \n",
    "    #dataset original + (100%, 200%, 300%) datos generados\n",
    "    x_aug_100 =np.concatenate((x_gan_100, x_train), axis=0)\n",
    "    y_aug_100 = np.concatenate((y_gan_100, y_train), axis=0)\n",
    "    \n",
    "    x_aug_200 =np.concatenate((x_gan_200, x_train), axis=0)\n",
    "    y_aug_200 = np.concatenate((y_gan_200, y_train), axis=0)\n",
    "    \n",
    "    x_aug_300 =np.concatenate((x_gan_300, x_train), axis=0)\n",
    "    y_aug_300 = np.concatenate((y_gan_300, y_train), axis=0)\n",
    "    \n",
    "    ## Desordeno los dataset mergeados \n",
    "    # x_aug_25 y y_aug_25\n",
    "    ''' shuffler = np.random.permutation(x_aug_25.shape[0])\n",
    "    x_aug_25 = x_aug_25[shuffler]\n",
    "    y_aug_25 = y_aug_25[shuffler]\n",
    "    print(\"x_aug_25 shape: {}, y_aug_25 shape:{}\".format(x_aug_25.shape,y_aug_25.shape))'''\n",
    "\n",
    "    # x_aug_50 y y_aug_50\n",
    "    shuffler = np.random.permutation(x_aug_50.shape[0])\n",
    "    x_aug_50 = x_aug_50[shuffler]\n",
    "    y_aug_50 = y_aug_50[shuffler]\n",
    "    print(\"x_aug_50 shape: {}, y_aug_50 shape:{}\".format(x_aug_50.shape,y_aug_50.shape))\n",
    "    \n",
    "    # x_aug_75 y y_aug_75\n",
    "    shuffler = np.random.permutation(x_aug_75.shape[0])\n",
    "    x_aug_75 = x_aug_75[shuffler]\n",
    "    y_aug_75 = y_aug_75[shuffler]\n",
    "    print(\"x_aug_75 shape: {}, y_aug_75 shape:{}\".format(x_aug_75.shape,y_aug_75.shape))\n",
    "    \n",
    "    # x_aug_100 y y_aug_100\n",
    "    shuffler = np.random.permutation(x_aug_100.shape[0])\n",
    "    x_aug_100 = x_aug_100[shuffler]\n",
    "    y_aug_100 = y_aug_100[shuffler]\n",
    "    print(\"x_aug_100 shape: {}, y_aug_100 shape:{}\".format(x_aug_100.shape,y_aug_100.shape))\n",
    "    \n",
    "    # x_aug_200 y y_aug_200\n",
    "    shuffler = np.random.permutation(x_aug_200.shape[0])\n",
    "    x_aug_200 = x_aug_200[shuffler]\n",
    "    y_aug_200 = y_aug_200[shuffler]\n",
    "    print(\"x_aug_200 shape: {}, y_aug_200 shape:{}\".format(x_aug_200.shape,y_aug_200.shape))\n",
    "    \n",
    "    # x_aug_300 y y_aug_300\n",
    "    shuffler = np.random.permutation(x_aug_300.shape[0])\n",
    "    x_aug_300 = x_aug_300[shuffler]\n",
    "    y_aug_300 = y_aug_300[shuffler]\n",
    "    print(\"x_aug_300 shape: {}, y_aug_300 shape:{}\".format(x_aug_300.shape,y_aug_300.shape))\n",
    "\n",
    "    #Guardamos los datos mergeados\n",
    "    np.save(numpy_data_path+'x_train',x_train)\n",
    "    np.save(numpy_data_path+'y_train',y_train)\n",
    "    '''np.save(numpy_data_path+'x_aug_25',x_aug_25)\n",
    "    np.save(numpy_data_path+'y_aug_25',y_aug_25)'''\n",
    "    np.save(numpy_data_path+'x_aug_50',x_aug_50)\n",
    "    np.save(numpy_data_path+'y_aug_50',y_aug_50)\n",
    "    np.save(numpy_data_path+'x_aug_75',x_aug_75)\n",
    "    np.save(numpy_data_path+'y_aug_75',y_aug_75)\n",
    "    np.save(numpy_data_path+'x_aug_100',x_aug_100)\n",
    "    np.save(numpy_data_path+'y_aug_100',y_aug_100)\n",
    "    np.save(numpy_data_path+'x_aug_200',x_aug_200)\n",
    "    np.save(numpy_data_path+'y_aug_200',y_aug_200)\n",
    "    np.save(numpy_data_path+'x_aug_300',x_aug_300)\n",
    "    np.save(numpy_data_path+'y_aug_300',y_aug_300)\n",
    "    np.save(numpy_data_path+'x_gan_100',x_gan_100)\n",
    "    np.save(numpy_data_path+'y_gan_100',y_gan_100)\n",
    "    np.save(numpy_data_path+'x_gan_200',x_gan_200)\n",
    "    np.save(numpy_data_path+'y_gan_200',y_gan_200)\n",
    "    np.save(numpy_data_path+'x_gan_300',x_gan_300)\n",
    "    np.save(numpy_data_path+'y_gan_300',y_gan_300)\n",
    "\n",
    "    #limpiamos memoria\n",
    "    torch.cuda.empty_cache()\n",
    "    del generator, x_train, y_train, x_aug_25, y_aug_25, x_aug_50, y_aug_50, x_aug_75, y_aug_75\n",
    "    del x_aug_100, y_aug_100, x_aug_200, y_aug_200, x_aug_300, y_aug_300, x_gan_100, y_gan_100\n",
    "    del x_gan_200, y_gan_200, x_gan_300, y_gan_300"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import  pytorch_fid_wrapper as pfw\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import handshape_datasets as hd\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader \n",
    "from PIL import Image\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(data.Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=128):\n",
    "        super(Dataloader, self).__init__()\n",
    "        self.x = dataset[0]\n",
    "        self.y = dataset[1]\n",
    "        self.len = dataset[0].shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.act_idx = 0\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        print(index)\n",
    "        if (index+self.batch_size <= self.len):\n",
    "            x = torch.Tensor(self.x[index:index+self.batch_size,:,:,:])  # batch dim is handled by the data loader\n",
    "            y = torch.Tensor(self.y[index:index+self.batch_size,]) \n",
    "        else:\n",
    "            x2 = torch.Tensor(self.x[index:self.len,:,:,:])  # batch dim is handled by the data loader\n",
    "            y2 = torch.Tensor(self.y[index:self.len,]) \n",
    "            \n",
    "        return x.permute(0,3, 1, 2), y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def netx(self):\n",
    "        if (self.act_idx+self.batch_size <= self.len):\n",
    "            x = torch.Tensor(self.x[self.act_idx:(self.act_idx+self.batch_size),:,:,:]) \n",
    "            y = torch.Tensor(self.y[self.act_idx:(self.act_idx+self.batch_size),])\n",
    "            self.act_idx += self.batch_size\n",
    "        else:\n",
    "            x = torch.Tensor(self.x[self.act_idx:self.len-1,:,:,:]) \n",
    "            y = torch.Tensor(self.y[self.act_idx:self.len-1,])\n",
    "            self.act_idx = self.len\n",
    "\n",
    "        return x.permute(0,3, 1, 2), y\n",
    "    \n",
    "    def reset_index(self):\n",
    "        self.act_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, transform=None):\n",
    "        super().__init__()\n",
    "        self.x = dataset[0]\n",
    "        self.y = dataset[1]\n",
    "        self.len = dataset[0].shape[0]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x[index]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \n",
    "    data = hd.load('PugeaultASL_A')\n",
    "\n",
    "    good_min = 40\n",
    "    good_classes = []\n",
    "    n_unique = len(np.unique(data[1]['y']))\n",
    "    for i in range(n_unique):\n",
    "        images = data[0][np.equal(i, data[1]['y'])]\n",
    "        if len(images) >= good_min:\n",
    "            good_classes = good_classes + [i]\n",
    "\n",
    "    x = data[0][np.in1d(data[1]['y'], good_classes)]\n",
    "    img_shape = x[0].shape\n",
    "    print(img_shape)\n",
    "    y = data[1]['y'][np.in1d(data[1]['y'], good_classes)]\n",
    "    y_dict = dict(zip(np.unique(y), range(len(np.unique(y)))))\n",
    "    y = np.vectorize(y_dict.get)(y)\n",
    "    classes = np.unique(y)\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    #escalo al rango  [1-,1]\n",
    "    #x = (x.astype('float32') -127.5 ) / 127.5\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, train_size=0.8, test_size=0.2, stratify=y)\n",
    "\n",
    "    #Reduciendo dataset\n",
    "    reduce = None\n",
    "    if reduce:\n",
    "        for i in range(n_classes):\n",
    "            images = x_train[np.equal(i, y_train)]\n",
    "            #x_reduce = x_reduce + images[0:(len(images)//2),:,:,:]\n",
    "            if i==0:\n",
    "                x_reduce = images[0:(len(images)//reduce),:,:,:]\n",
    "                y_reduce = np.ones((len(images)//reduce)) * i\n",
    "            else:\n",
    "                x_reduce = np.concatenate((x_reduce, images[0:(len(images)//reduce),:,:,:]), axis=0)\n",
    "                y_temp = np.ones((len(images)//reduce)) * i\n",
    "                y_reduce = np.concatenate((y_reduce, y_temp), axis=0)\n",
    "\n",
    "        print('x.shpae: {}, y.shape: {}'.format(x.shape, y.shape))\n",
    "        print('x_reduce.shpae: {}, y_reduce.shape: {}'.format(x_reduce.shape, y_reduce.shape))\n",
    "\n",
    "        #Desordeno el nuevo dataset\n",
    "        shuffler = np.random.permutation(x_reduce.shape[0])\n",
    "        x_reduce = x_reduce[shuffler]\n",
    "        y_reduce = y_reduce[shuffler]\n",
    "        print(\"dataset final shape: {}, y final:{}\".format(x_reduce.shape,y_reduce.shape))\n",
    "\n",
    "\n",
    "    train_size = x_train.shape[0]\n",
    "    test_size = x_test.shape[0]\n",
    "\n",
    "    return n_classes, x_train, y_train, x_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_with_subject(subject_test=1):\n",
    "    \n",
    "    data = hd.load('PugeaultASL_A')\n",
    "\n",
    "    good_min = 40\n",
    "    good_classes = []\n",
    "    n_unique = len(np.unique(data[1]['y']))\n",
    "    for i in range(n_unique):\n",
    "        images = data[0][np.equal(i, data[1]['y'])]\n",
    "        if len(images) >= good_min:\n",
    "            good_classes = good_classes + [i]\n",
    "\n",
    "    x = data[0][np.in1d(data[1]['y'], good_classes)]\n",
    "\n",
    "    y = data[1]['y'][np.in1d(data[1]['y'], good_classes)]\n",
    "\n",
    "    s = data[1]['subjects'][np.in1d(data[1]['y'], good_classes)]\n",
    "\n",
    "    y_dict = dict(zip(np.unique(y), range(len(np.unique(y)))))\n",
    "    y = np.vectorize(y_dict.get)(y)\n",
    "\n",
    "    s_dict = dict(zip(np.unique(s), range(len(np.unique(s)))))\n",
    "    s = np.vectorize(s_dict.get)(s)\n",
    "\n",
    "    classes = np.unique(y)\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    x_train = x[np.not_equal(subject_test, s)]\n",
    "    y_train = y[np.not_equal(subject_test, s)]\n",
    "    x_test = x[np.equal(subject_test, s)]\n",
    "    y_test = y[np.equal(subject_test, s)]\n",
    "    \n",
    "    shuffler = np.random.permutation(x_train.shape[0])\n",
    "    x_train = x_train[shuffler]\n",
    "    y_train = y_train[shuffler]\n",
    "\n",
    "    shuffler_test = np.random.permutation(x_test.shape[0])\n",
    "    x_test = x_test[shuffler_test]\n",
    "    y_test = y_test[shuffler_test]\n",
    "    \n",
    "    #escalo al rango  [1-,1]\n",
    "    #x_train = (x_train.astype('float32') -127.5 ) / 127.5\n",
    "    #x_test = (x_test.astype('float32') -127.5 ) / 127.5\n",
    "\n",
    "\n",
    "    return n_classes, x_train, y_train, x_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lsa16_rotated():\n",
    "    images =[]\n",
    "    labels = []\n",
    "    i=0\n",
    "    path = \"/home/willys/tesis/Data-augmentation-using-GANs/datasets/rgb_black_background/\"\n",
    "    for filename in glob.glob(path+'*.png'):\n",
    "        #append iamge\n",
    "        image = Image.open(os.path.join(path, filename))\n",
    "        image_to_numpy=np.asarray(image)\n",
    "        images.append(image_to_numpy)\n",
    "        #create label and append\n",
    "        image_name = filename.split('/')[-1]\n",
    "        label = (int(image_name.split('_')[0]))\n",
    "        labels.append(label-1)\n",
    "\n",
    "    images = np.asarray(images)\n",
    "    labels = np.asarray(labels)\n",
    "    n_classes = len(np.unique(labels))\n",
    "    \n",
    "    #Desordeno el nuevo dataset\n",
    "    shuffler = np.random.permutation(images.shape[0])\n",
    "    images = images[shuffler]\n",
    "    labels = labels[shuffler]\n",
    "    \n",
    "    #escalo al rango  [1-,1]\n",
    "    images = (images.astype('float32') -127.5 ) / 127.5\n",
    "    #split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        images, labels, train_size=0.8, test_size=0.2, stratify=labels)\n",
    "    \n",
    "    return n_classes, x_train, y_train, x_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lsa16_rotated_with_subject(subject_test=1):\n",
    "    x_train =[]\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    i=0\n",
    "    path = \"/home/willys/tesis/Data-augmentation-using-GANs/datasets/rgb_black_background/\"\n",
    "    for filename in glob.glob(path+'*.png'):\n",
    "        #append iamge\n",
    "        image = Image.open(os.path.join(path, filename))\n",
    "        image_to_numpy=np.asarray(image)\n",
    "        \n",
    "        # get label and subject\n",
    "        image_name = filename.split('/')[-1]\n",
    "        label = (int(image_name.split('_')[0]))\n",
    "        subject = (int(image_name.split('_')[1]))\n",
    "        \n",
    "        if subject==subject_test:\n",
    "            x_test.append(image_to_numpy)\n",
    "            y_test.append(label-1)\n",
    "        else:\n",
    "            x_train.append(image_to_numpy)\n",
    "            y_train.append(label-1)        \n",
    "\n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    x_test = np.asarray(x_test)\n",
    "    y_test = np.asarray(y_test)\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    \n",
    "    #Desordeno los datos de train\n",
    "    shuffler_train = np.random.permutation(x_train.shape[0])\n",
    "    x_train = x_train[shuffler_train]\n",
    "    y_train = y_train[shuffler_train]\n",
    "    \n",
    "    #Desordeno los datos de train\n",
    "    shuffler_test = np.random.permutation(x_test.shape[0])\n",
    "    x_test = x_test[shuffler_test]\n",
    "    y_test = y_test[shuffler_test]\n",
    "    \n",
    "    #escalo al rango  [1-,1]\n",
    "    x_train = (x_train.astype('float32') -127.5 ) / 127.5\n",
    "    x_test = (x_test.astype('float32') -127.5 ) / 127.5\n",
    "    \n",
    "    return n_classes, x_train, y_train, x_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonal_regularization(weight):\n",
    "    '''\n",
    "    Function for computing the orthogonal regularization term for a given weight matrix.\n",
    "    '''\n",
    "    weight = weight.flatten(1)\n",
    "    return torch.norm(\n",
    "        torch.dot(weight, weight) * (torch.ones_like(weight) - torch.eye(weight.shape[0]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=16, size=(3, 32, 32), nrow=4, show=True, save=False, \n",
    "                       path=''):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassConditionalBatchNorm2d(nn.Module):\n",
    "    '''\n",
    "    ClassConditionalBatchNorm2d Class\n",
    "    Values:\n",
    "    in_channels: the dimension of the class embedding (c) + noise vector (z), a scalar\n",
    "    out_channels: the dimension of the activation tensor to be normalized, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
    "        self.class_scale_transform = nn.utils.spectral_norm(nn.Linear(in_channels, out_channels, bias=False))\n",
    "        self.class_shift_transform = nn.utils.spectral_norm(nn.Linear(in_channels, out_channels, bias=False))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        normalized_image = self.bn(x)\n",
    "        class_scale = (1 + self.class_scale_transform(y))[:, :, None, None]\n",
    "        class_shift = self.class_shift_transform(y)[:, :, None, None]\n",
    "        transformed_image = class_scale * normalized_image + class_shift\n",
    "        return transformed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    '''\n",
    "    AttentionBlock Class\n",
    "    Values:\n",
    "    channels: number of channels in input\n",
    "    '''\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "\n",
    "        self.theta = nn.utils.spectral_norm(nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=False))\n",
    "        self.phi = nn.utils.spectral_norm(nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=False))\n",
    "        self.g = nn.utils.spectral_norm(nn.Conv2d(channels, channels // 2, kernel_size=1, padding=0, bias=False))\n",
    "        self.o = nn.utils.spectral_norm(nn.Conv2d(channels // 2, channels, kernel_size=1, padding=0, bias=False))\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        spatial_size = x.shape[2] * x.shape[3]\n",
    "\n",
    "        # Apply convolutions to get query (theta), key (phi), and value (g) transforms\n",
    "        theta = self.theta(x)\n",
    "        phi = F.max_pool2d(self.phi(x), kernel_size=2)\n",
    "        g = F.max_pool2d(self.g(x), kernel_size=2)\n",
    "\n",
    "        # Reshape spatial size for self-attention\n",
    "        theta = theta.view(-1, self.channels // 8, spatial_size)\n",
    "        phi = phi.view(-1, self.channels // 8, spatial_size // 4)\n",
    "        g = g.view(-1, self.channels // 2, spatial_size // 4)\n",
    "\n",
    "        # Compute dot product attention with query (theta) and key (phi) matrices\n",
    "        beta = F.softmax(torch.bmm(theta.transpose(1, 2), phi), dim=-1)\n",
    "\n",
    "        # Compute scaled dot product attention with value (g) and attention (beta) matrices\n",
    "        o = self.o(torch.bmm(g, beta.transpose(1, 2)).view(-1, self.channels // 2, x.shape[2], x.shape[3]))\n",
    "\n",
    "        # Apply gain and residual\n",
    "        return self.gamma * o + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GResidualBlock(nn.Module):\n",
    "    '''\n",
    "    GResidualBlock Class\n",
    "    Values:\n",
    "    c_dim: the dimension of conditional vector [c, z], a scalar\n",
    "    in_channels: the number of channels in the input, a scalar\n",
    "    out_channels: the number of channels in the output, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, c_dim, in_channels, out_channels, scale_factor=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "        self.bn1 = ClassConditionalBatchNorm2d(c_dim, in_channels)\n",
    "        self.bn2 = ClassConditionalBatchNorm2d(c_dim, out_channels)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.upsample_fn = nn.Upsample(scale_factor=scale_factor) # upsample occurs in every gblock\n",
    "\n",
    "        self.mixin = (in_channels != out_channels)\n",
    "        if self.mixin:\n",
    "            self.conv_mixin = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # h := upsample(x, y)\n",
    "        h = self.bn1(x, y)\n",
    "        h = self.activation(h)\n",
    "        h = self.upsample_fn(h)\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        # h := conv(h, y)\n",
    "        h = self.bn2(h, y)\n",
    "        h = self.activation(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        # x := upsample(x)\n",
    "        x = self.upsample_fn(x)\n",
    "        if self.mixin:\n",
    "            x = self.conv_mixin(x)\n",
    "\n",
    "        return h + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "    z_dim: the dimension of random noise sampled, a scalar\n",
    "    shared_dim: the dimension of shared class embeddings, a scalar\n",
    "    base_channels: the number of base channels, a scalar\n",
    "    bottom_width: the height/width of image before it gets upsampled, a scalar\n",
    "    n_classes: the number of image classes, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base_channels=96, bottom_width=4, z_dim=120, shared_dim=128, n_classes=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        n_chunks = 5    # 4 (generator blocks) + 1 (generator input)\n",
    "        self.z_chunk_size = z_dim // n_chunks\n",
    "        self.z_dim = z_dim\n",
    "        self.shared_dim = shared_dim\n",
    "        self.bottom_width = bottom_width\n",
    "\n",
    "        # No spectral normalization on embeddings, which authors observe to cripple the generator\n",
    "        self.shared_emb = nn.Embedding(n_classes, shared_dim)\n",
    "\n",
    "        self.proj_z = nn.Linear(self.z_chunk_size, 16 * base_channels * bottom_width ** 2)\n",
    "\n",
    "        # Can't use one big nn.Sequential since we are adding class+noise at each block\n",
    "        self.g_blocks = nn.ModuleList([\n",
    "\n",
    "            nn.ModuleList([\n",
    "                GResidualBlock(shared_dim + self.z_chunk_size, 16 * base_channels, 8 * base_channels),\n",
    "                AttentionBlock(8 * base_channels),\n",
    "            ]),\n",
    "             nn.ModuleList([\n",
    "                GResidualBlock(shared_dim + self.z_chunk_size, 8 * base_channels, 4 * base_channels),\n",
    "                AttentionBlock(4 * base_channels),\n",
    "            ]),\n",
    "            nn.ModuleList([\n",
    "                GResidualBlock(shared_dim + self.z_chunk_size, 4 * base_channels, 2 * base_channels),\n",
    "                AttentionBlock(2 * base_channels),\n",
    "            ]),\n",
    "            nn.ModuleList([\n",
    "                GResidualBlock(shared_dim + self.z_chunk_size, 2 * base_channels, base_channels, 1),\n",
    "                AttentionBlock(base_channels),\n",
    "            ]),\n",
    "        ])\n",
    "        self.proj_o = nn.Sequential(\n",
    "            nn.BatchNorm2d(base_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.utils.spectral_norm(nn.Conv2d(base_channels, 3, kernel_size=1, padding=0)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def loss (self, disc_fake_pred):\n",
    "        \n",
    "        return - disc_fake_pred.mean()\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        '''\n",
    "        z: random noise with size self.z_dim\n",
    "        y: class embeddings with size self.shared_dim\n",
    "            = NOTE =\n",
    "            y should be class embeddings from self.shared_emb, not the raw class labels\n",
    "        '''\n",
    "        # Chunk z and concatenate to shared class embeddings\n",
    "        zs = torch.split(z, self.z_chunk_size, dim=1)\n",
    "        z = zs[0]\n",
    "        ys = [torch.cat([y, z], dim=1) for z in zs[1:]]\n",
    "\n",
    "        # Project noise and reshape to feed through generator blocks\n",
    "        h = self.proj_z(z)\n",
    "        h = h.view(h.size(0), -1, self.bottom_width, self.bottom_width)\n",
    "\n",
    "        # Feed through generator blocks\n",
    "        for idx, g_block in enumerate(self.g_blocks):\n",
    "            h = g_block[0](h, ys[idx])\n",
    "            h = g_block[1](h)\n",
    "\n",
    "        # Project to 3 RGB channels with tanh to map values to [-1, 1]\n",
    "        h = self.proj_o(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DResidualBlock(nn.Module):\n",
    "    '''\n",
    "    DResidualBlock Class\n",
    "    Values:\n",
    "    in_channels: the number of channels in the input, a scalar\n",
    "    out_channels: the number of channels in the output, a scalar\n",
    "    downsample: whether to apply downsampling\n",
    "    use_preactivation: whether to apply an activation function before the first convolution\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, downsample=True, use_preactivation=False, \n",
    "                 downsample_scale=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.use_preactivation = use_preactivation  # apply preactivation in all except first dblock\n",
    "\n",
    "        self.downsample = downsample    # downsample occurs in all except last dblock\n",
    "        if downsample:\n",
    "            self.downsample_fn = nn.AvgPool2d(downsample_scale)\n",
    "        self.mixin = (in_channels != out_channels) or downsample\n",
    "        if self.mixin:\n",
    "            self.conv_mixin = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0))\n",
    "\n",
    "    def _residual(self, x):\n",
    "        if self.use_preactivation:\n",
    "            if self.mixin:\n",
    "                x = self.conv_mixin(x)\n",
    "            if self.downsample:\n",
    "                x = self.downsample_fn(x)\n",
    "        else:\n",
    "            if self.downsample:\n",
    "                x = self.downsample_fn(x)\n",
    "            if self.mixin:\n",
    "                x = self.conv_mixin(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply preactivation if applicable\n",
    "        if self.use_preactivation:\n",
    "            h = F.relu(x)\n",
    "        else:\n",
    "            h = x\n",
    "\n",
    "        h = self.conv1(h)\n",
    "        h = self.activation(h)\n",
    "        if self.downsample:\n",
    "            h = self.downsample_fn(h)\n",
    "\n",
    "        return h + self._residual(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "    base_channels: the number of base channels, a scalar\n",
    "    n_classes: the number of image classes, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base_channels=96, n_classes=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        # For adding class-conditional evidence\n",
    "        self.shared_emb = nn.utils.spectral_norm(nn.Embedding(n_classes, 8 * base_channels))\n",
    "\n",
    "        self.d_blocks = nn.Sequential(\n",
    "            DResidualBlock(3, base_channels, downsample=True, use_preactivation=False),\n",
    "            AttentionBlock(base_channels),\n",
    "\n",
    "            DResidualBlock(base_channels, 2 * base_channels, downsample=True, use_preactivation=True),\n",
    "            AttentionBlock(2 * base_channels),\n",
    "\n",
    "            DResidualBlock(2 * base_channels, 4 * base_channels, downsample=True, use_preactivation=True,\n",
    "                          downsample_scale=2),\n",
    "            AttentionBlock(4 * base_channels),\n",
    "            \n",
    "            DResidualBlock(4 * base_channels, 8 * base_channels, downsample=True, use_preactivation=True,\n",
    "                          downsample_scale=1),\n",
    "            AttentionBlock(8 * base_channels),\n",
    "\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.proj_o = nn.utils.spectral_norm(nn.Linear(8 * base_channels, 1))\n",
    "        \n",
    "    def loss(self, disc_real_pred, disc_fake_pred):\n",
    "        \n",
    "        d_loss_fake = torch.nn.ReLU()(1.0 + disc_fake_pred).mean()\n",
    "        d_loss_real = torch.nn.ReLU()(1.0 - disc_real_pred).mean()\n",
    "        \n",
    "        return d_loss_real + d_loss_fake\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        h = self.d_blocks(x)\n",
    "        h = torch.sum(h, dim=[2, 3])\n",
    "\n",
    "        # Class-unconditional output\n",
    "        uncond_out = self.proj_o(h)\n",
    "        if y is None:\n",
    "            return uncond_out\n",
    "\n",
    "        # Class-conditional output\n",
    "        cond_out = torch.sum(self.shared_emb(y) * h, dim=1, keepdim=True)\n",
    "        \n",
    "        return uncond_out + cond_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, base_channels, z_dim, shared_dim, n_classes,\n",
    "          generator, discriminator, gen_opt, disc_opt, epochs, weights_dir, summary_fid):\n",
    "    cur_step = 0\n",
    "    min_fids = np.array([2000,2000,2000])\n",
    "    e = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('##############################')\n",
    "        print('#epoch: {}'.format(epoch))\n",
    "        print('##############################')\n",
    "        \n",
    "        for batch_ndx, sample in enumerate(loader):\n",
    "            real, labels = sample[0], sample[1]\n",
    "            batch_size = len(real)\n",
    "            real = real.to(device)\n",
    "            \n",
    "            for i in range(2):\n",
    "                ### Update discriminator ###\n",
    "                # Zero out the discriminator gradients\n",
    "                disc_opt.zero_grad()\n",
    "                # Get noise corresponding to the current batch_size \n",
    "                z = torch.randn(batch_size, z_dim, device=device)       # Generate random noise (z)\n",
    "                y = labels.to(device).long()    # Generate a batch of labels (y), one for each class\n",
    "                y_emb = generator.shared_emb(y)                         # Retrieve class embeddings (y_emb) from generator\n",
    "                fake = generator(z, y_emb)\n",
    "\n",
    "                disc_fake_pred = discriminator(fake.detach(), y)  \n",
    "                disc_real_pred = discriminator(real, y)  \n",
    "\n",
    "                #loss\n",
    "                disc_loss = discriminator.loss(disc_real_pred, disc_fake_pred)\n",
    "                # Update gradients\n",
    "                disc_loss.backward(retain_graph=True)\n",
    "                # Update optimizer\n",
    "                disc_opt.step()\n",
    "\n",
    "\n",
    "\n",
    "            ### Update generator ###\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            fake = generator(z, y_emb)\n",
    "            disc_fake_pred = discriminator(fake, y)  \n",
    "            #loss\n",
    "            gen_loss =  generator.loss(disc_fake_pred)\n",
    "            # Update gradients\n",
    "            gen_loss.backward()\n",
    "            # Update optimizer\n",
    "            gen_opt.step()\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            cur_step +=1\n",
    "\n",
    "            if cur_step % 100 == 0:\n",
    "                print('===========================================================================')\n",
    "                try:\n",
    "                    val_fid = pfw.fid(fake, real_images=real, device=\"cuda:0\")\n",
    "                    message = 'FID in step_{}: {}'.format(cur_step, val_fid)\n",
    "                    print(message)\n",
    "                    print(message, file=summary_fid)\n",
    "                    \n",
    "                    if (val_fid < min_fids).any():\n",
    "                        idx = min_fids.argmax()\n",
    "                        min_fids[idx] = val_fid\n",
    "                        weights_dir_specific = weights_dir+'weights_{}/'.format(idx)\n",
    "                        if not os.path.exists(weights_dir_specific):\n",
    "                            os.makedirs(weights_dir_specific)\n",
    "                        torch.save(generator.state_dict(), weights_dir_specific+'gen.state_dict')\n",
    "                        path_image = weights_dir_specific+'images_gen.png'\n",
    "                        show_tensor_images(fake, show=False, save=True, path=path_image)\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                show_tensor_images(real)\n",
    "                path_data_gen = '../../../../data_gen/BigGAN/PugeaultASL_A/image_step_{}'.format(cur_step) \n",
    "                show_tensor_images(fake, save=True,path=path_data_gen)\n",
    "                print('===========================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in range(0,3):\n",
    "    device = 'cuda'\n",
    "    #charge data\n",
    "    n_classes, x_train, y_train, _, _ = load_dataset_with_subject(subject_test=subject)\n",
    "    \n",
    "    transformOpt = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomApply(torch.nn.ModuleList([\n",
    "                                       transforms.ColorJitter(brightness=0.5, \n",
    "                                                               contrast=0.5, \n",
    "                                                               saturation=0.5, \n",
    "                                                               hue=0.2),\n",
    "                                    ]), p=0.5),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            np.array,\n",
    "            lambda x:((x.astype('float32') -127.5 ) / 127.5),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    dataset = CustomDataset((x_train,y_train), transform=transformOpt)\n",
    "    batch_size = 64\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, num_workers=4)\n",
    "    \n",
    "    #Creo los paths \n",
    "    weights_dir = 'generators_weights/PugeaultASL_A_transforms/subject_{}/'.format(subject)\n",
    "    if not os.path.exists(weights_dir):\n",
    "        os.makedirs(weights_dir)\n",
    "    data_dir = 'numpy_data/PugeaultASL_A_transforms/subject_{}/'.format(subject)\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    #Creo un txt para registrar los FID\n",
    "    summary_fid = open(weights_dir+\"summary_fid.txt\", \"a\")\n",
    "\n",
    "    # Initialize models\n",
    "    base_channels = 96\n",
    "    z_dim = 120  \n",
    "    shared_dim = 128\n",
    "    generator = Generator(base_channels=base_channels, bottom_width=4, z_dim=z_dim, shared_dim=shared_dim, n_classes=n_classes).to(device)\n",
    "    discriminator = Discriminator(base_channels=base_channels, n_classes=n_classes).to(device)\n",
    "\n",
    "    # Initialize weights orthogonally\n",
    "    for module in generator.modules():\n",
    "        if (isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, nn.Embedding)):\n",
    "            nn.init.orthogonal_(module.weight)\n",
    "    for module in discriminator.modules():\n",
    "        if (isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, nn.Embedding)):\n",
    "            nn.init.orthogonal_(module.weight)\n",
    "\n",
    "    # Initialize optimizers\n",
    "    gen_opt = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.0, 0.999), eps=1e-6)\n",
    "    disc_opt = torch.optim.Adam(discriminator.parameters(), lr=4e-4, betas=(0.0, 0.999), eps=1e-6)\n",
    "    \n",
    "    train(loader, base_channels, z_dim, shared_dim, n_classes,\n",
    "          generator, discriminator, gen_opt, disc_opt, 30, weights_dir, summary_fid)\n",
    "    \n",
    "    summary_fid.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-income",
   "metadata": {},
   "source": [
    "## Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in range(0,1):\n",
    "    print('##############')\n",
    "    print('#  subject {} #'.format(q))\n",
    "    print('##############')\n",
    "    weights_path = 'generators_weights/PugeaultASL_A_transforms/subject_{}/'.format(q)\n",
    "    numpy_data_path = 'numpy_data/PugeaultASL_A_transforms/subject_{}/'.format(q)\n",
    "    if not os.path.exists(numpy_data_path):\n",
    "        os.makedirs(numpy_data_path)\n",
    "    total_images = 1792\n",
    "    batch_size = 64\n",
    "    n_classes, x_train, y_train, _, _ = load_dataset_with_subject(subject_test=q)\n",
    "    # Initialize models\n",
    "    base_channels = 96\n",
    "    z_dim = 120  \n",
    "    shared_dim = 128\n",
    "    device = 'cuda'\n",
    "    generator = Generator(base_channels=base_channels, bottom_width=4, \n",
    "                          z_dim=z_dim, shared_dim=shared_dim, n_classes=n_classes).to(device)\n",
    "    \n",
    "    #100% imagenes generadas\n",
    "    x_aug_100 = []\n",
    "    y_aug_100 = []\n",
    "    \n",
    "    for k in range(3):\n",
    "        specific_weights_path = weights_path+('weights_{}/gen.state_dict'.format(k))\n",
    "        generator.load_state_dict(torch.load(specific_weights_path))\n",
    "        generator.eval()\n",
    "        \n",
    "        for j in range(total_images//batch_size):\n",
    "            for i in range(n_classes):\n",
    "\n",
    "                #creo imagenes falsas y las mergeo\n",
    "                z = torch.randn(batch_size, z_dim, device=device)                 # Generate random noise (z)\n",
    "                y = (torch.ones(batch_size) * i).to(device).long()    # Generate a batch of labels (y), one for each class\n",
    "                y_emb = generator.shared_emb(y)                                  # Retrieve class embeddings (y_emb) from generator\n",
    "                fake = generator(z, y_emb)\n",
    "\n",
    "                #acomodo las dimensiones, escala y tipo\n",
    "                image_unflat = fake.detach().cpu()\n",
    "                final_images = image_unflat.permute(0,2, 3, 1).numpy()\n",
    "\n",
    "                x_aug_100.append(final_images)\n",
    "                y_tmp = np.ones(batch_size,)* i\n",
    "                y_aug_100.append(y_tmp)\n",
    "                    \n",
    "    \n",
    "    #dataset 100% GAN\n",
    "    x_aug_100 = (np.asarray(x_aug_100)).reshape((-1,32,32,3))\n",
    "    y_aug_100 = (np.asarray(y_aug_100)).reshape((-1))\n",
    "    \n",
    "    #dataset 50/50\n",
    "    x_aug_50, _, y_aug_50, _ = train_test_split(\n",
    "        x_aug_100, y_aug_100, train_size=0.5, test_size=0.5, stratify=y_aug_100)\n",
    "    \n",
    "    x_aug_50 =np.concatenate((x_aug_50, x_train), axis=0)\n",
    "    y_aug_50 = np.concatenate((y_aug_50, y_train), axis=0)\n",
    "    \n",
    "    #dataset 75/25 (GAN/normal)\n",
    "    '''Como el dataset normal tiene la mitad de datos que x_aug_100, el 25% de datos normales \n",
    "    con respecto al dataset GAN equivale a la mitad del dataset original'''\n",
    "    x_aug_75, _, y_aug_75, _ = train_test_split(\n",
    "        x_aug_100, y_aug_100, train_size=0.75, test_size=0.25, stratify=y_aug_100)\n",
    "    \n",
    "    x_train_25, _, y_train_25, _ = train_test_split(\n",
    "        x_train, y_train, train_size=0.5, test_size=0.5, stratify=y_train)\n",
    "    \n",
    "    x_aug_75 =np.concatenate((x_aug_75, x_train_25), axis=0)\n",
    "    y_aug_75 = np.concatenate((y_aug_75, y_train_25), axis=0)\n",
    "    \n",
    "    #dataset 25/75 (GAN/normal)\n",
    "    '''Como el dataset normal tiene la mitad de datos que x_aug_100, el 25% de datos GAN \n",
    "    con respecto al dataset original es del 12.5%'''\n",
    "    x_aug_25, _, y_aug_25, _ = train_test_split(\n",
    "        x_aug_100, y_aug_100, train_size=0.125, test_size=0.875, stratify=y_aug_100)\n",
    "    \n",
    "    x_train_75, _, y_train_75, _ = train_test_split(\n",
    "        x_train, y_train, train_size=0.75, test_size=0.25, stratify=y_train)\n",
    "    \n",
    "    x_aug_25 =np.concatenate((x_aug_25, x_train_75), axis=0)\n",
    "    y_aug_25 = np.concatenate((y_aug_25, y_train_75), axis=0)\n",
    "    \n",
    "    #Desordeno el nuevo dataset 100% GAN\n",
    "    shuffler = np.random.permutation(x_aug_100.shape[0])\n",
    "    x_aug_100 = x_aug_100[shuffler]\n",
    "    y_aug_100 = y_aug_100[shuffler]\n",
    "    print(\" dataset 100% GAN shape: {}, y final:{}\".format(x_aug_100.shape,y_aug_100.shape))\n",
    "\n",
    "    #Desordeno el nuevo dataset 50/50\n",
    "    shuffler = np.random.permutation(x_aug_50.shape[0])\n",
    "    x_aug_50 = x_aug_50[shuffler]\n",
    "    y_aug_50 = y_aug_50[shuffler]\n",
    "    print(\" dataset 50/50 shape: {}, y final:{}\".format(x_aug_50.shape,y_aug_50.shape))\n",
    "\n",
    "    #Desordeno el nuevo dataset 75/25 (GAN/normal)\n",
    "    shuffler = np.random.permutation(x_aug_75.shape[0])\n",
    "    x_aug_75 = x_aug_75[shuffler]\n",
    "    y_aug_75 = y_aug_75[shuffler]\n",
    "    print(\" dataset 75/25 shape: {}, y final:{}\".format(x_aug_75.shape,y_aug_75.shape))\n",
    "\n",
    "    #Desordeno el nuevo dataset 25/75 (GAN/normal)\n",
    "    shuffler = np.random.permutation(x_aug_25.shape[0])\n",
    "    x_aug_25 = x_aug_25[shuffler]\n",
    "    y_aug_25 = y_aug_25[shuffler]\n",
    "    print(\" dataset 25/75 shape: {}, y final:{}\".format(x_aug_25.shape,y_aug_25.shape))\n",
    "\n",
    "    #Guardamos los datos mergeados\n",
    "    np.save(numpy_data_path+'x_aug_100',x_aug_100)\n",
    "    np.save(numpy_data_path+'y_aug_100',y_aug_100)\n",
    "    np.save(numpy_data_path+'x_aug_50',x_aug_50)\n",
    "    np.save(numpy_data_path+'y_aug_50',y_aug_50)\n",
    "    np.save(numpy_data_path+'x_aug_75',x_aug_75)\n",
    "    np.save(numpy_data_path+'y_aug_75',y_aug_75)\n",
    "    np.save(numpy_data_path+'x_aug_25',x_aug_25)\n",
    "    np.save(numpy_data_path+'y_aug_25',y_aug_25)\n",
    "\n",
    "    #limpiamos memoria\n",
    "    torch.cuda.empty_cache()\n",
    "    del generator, x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "\n",
    "#transform = transforms.RandomRotation(degrees=(90, -90))\n",
    "transformOpt = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomPerspective(distortion_scale=0.35),\n",
    "            transforms.RandomRotation(45),\n",
    "            transforms.ToTensor()\n",
    "            \n",
    "        ])\n",
    "n_classes, x_train, y_train, x_test, y_test = load_dataset()\n",
    "dataset = CustomDataset((x_train,y_train), transform=transformOpt)\n",
    "batch_size = 64\n",
    "loader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes, x_train, y_train, _, _ = load_dataset_with_subject(subject_test=1)\n",
    "\n",
    "transformOpt = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomApply(torch.nn.ModuleList([\n",
    "                                       transforms.ColorJitter(brightness=0.5, \n",
    "                                                               contrast=0.5, \n",
    "                                                               saturation=0.5, \n",
    "                                                               hue=0.1),\n",
    "                                    ]), p=0.5),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            #transforms.RandomRotation(15),\n",
    "            np.array,\n",
    "            lambda x:((x.astype('float32') -127.5 ) / 127.5),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "dataset = CustomDataset((x_train,y_train), transform=transformOpt)\n",
    "batch_size = 64\n",
    "loader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_ndx, sample in enumerate(loader):\n",
    "    real, labels = sample[0], sample[1]\n",
    "    show_tensor_images(real)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hd.load('indianB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-incidence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
